#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# %%
# https://www.inflearn.com/course/%EB%82%98%EB%8F%84%EC%BD%94%EB%94%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%8B%9C%EA%B0%81%ED%99%94
from timeit import timeit
from turtle import color
from matplotlib import legend
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# import seaborn as sns
import matplotlib as mpl

# í•œê¸€í™” ì‘ì—…
plt.figure(dpi=600) # ê·¸ë˜í”„ë¥¼ ì„ ëª…í•˜ê²Œ
plt.rc('font', family = 'NanumGothic') # ì‹œìŠ¤í…œì— í°íŠ¸ì„¤ì¹˜í›„, ì‹œìŠ¤í…œ ì¬ì‹œì‘
plt.rc('axes', unicode_minus = False) # í•œê¸€ í°íŠ¸ ì‚¬ìš©ì‹œ ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ê°€ ê¹¨ì§€ëŠ” ë¬¸ì œ í•´ê²°
plt.style.use('fivethirtyeight') # ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•´ ë´…ë‹ˆë‹¤.

# íŒë‹¤ìŠ¤ ìµœëŒ€ ì»¬ëŸ¼ ì§€ì • (ì»¬ëŸ¼ì— ... í‘œì‹œ ë°©ì§€)
pd.options.display.max_columns = 100
# retina ë””ìŠ¤í”Œë ˆì´ê°€ ì§€ì›ë˜ëŠ” í™˜ê²½ì—ì„œ ì‹œê°í™” í°íŠ¸ê°€ ì¢€ ë” ì„ ëª…í•´ ë³´ì…ë‹ˆë‹¤.
# from IPython.display import set_matplotlib_formats
# set_matplotlib_formats('retina')

# %%
# ì„ í˜•íšŒê·€ ì´ë¡ ê³¼ ì‹¤ìŠµ
# ì„ í˜•íšŒê·€ëŠ” í•˜ë‚˜ ì´ìƒì˜ íŠ¹ì„±ê³¼ ì—°ì†ì ì¸ íƒ€ê¹ƒ ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ê²ƒ
# ì—°ì†ì ì¸ ì¶œë ¥ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
# íŠ¹ì„±ì´ í•˜ë‚˜ì¸ ì„ í˜• ëª¨ë¸ ê³µì‹
# Y = W0 + W1*X ( y = a*x + b )
# where W0 : yì¶• ì ˆí¸, W1 : íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜
# âœ” ëª©ì  : íŠ¹ì„±ê³¼ íƒ€ê¹ƒ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ í˜• ë°©ì •ì‹ì˜ ê°€ì¤‘ì¹˜(W)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ

# W0ì™€ W1ì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ ëª¨ë¸ë§ ì´ë¼ê³  í•©ë‹ˆë‹¤.
# - ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ í›ˆë ¨ê³¼ ë¹„ìš©í•¨ìˆ˜
# ëª¨ë¸ì˜ í›ˆë ¨ì´ë€
# âœ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì˜ ë§ë„ë¡ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒ
# âœ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì–¼ë§ˆë‚˜ ì˜ ë“¤ì–´ë§ëŠ”ì§€ ì¸¡ì •í•´ì•¼ í•¨

# ëª¨ë¸ í›ˆë ¨ì— í•„ìš”í•œ ë¹„ìš©í•¨ìˆ˜ ì¢…ë¥˜
# âœ” MSE (Mean Squared Error)
# 1. íšŒê·€ ëª¨ë¸ì˜ ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜
# 2. ì°¸ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì¸ ì˜¤ì°¨ë“¤ì˜ ì œê³± í‰ê· ìœ¼ë¡œ ì •ì˜
# 3. ì œê³±ì„ í•´ì£¼ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜(outlier)ì— ë¯¼ê°

# %%
# Given values
Y_true = [1, 1, 2, 2, 4] # Y_true = Y (original value)

# Predict values
Y_pred = [0.6, 1.29, 1.99, 2.69, 3.4] # Y_pred = Y

# %%
def mse1(y_true, predictions):
  y_true, predictions = np.array(y_true), np.array(predictions)
  return np.square(np.subtract(y_true, prediction)).mean()

from sklearn.metrics import mean_squared_error
def mse(y_true, predictions):
  return mean_squared_error(y_true, predictions)

MSE1 = mse1(Y_true, Y_pred)
MSE2 = mse(Y_true, Y_pred)
print(MSE1)
print(MSE2)

# %%
# âœ” MAE (Mean Absolute Error)
# 1. ì°¸ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì¸ ì˜¤ì°¨ë“¤ì˜ ì ˆëŒ€ê°’ í‰ê· 
# 2. MSEë³´ë‹¤ ì´ìƒì¹˜ì— ëœ ë¯¼ê°

# Creating a custom function for MAE
def mae(y_true, predictions):
    y_true, predictions = np.array(y_true), np.array(predictions)
    return np.mean(np.abs(y_true - predictions))

MAE = mae(Y_true, Y_pred)
MAE

# %%
# âœ” RMSE (Root Mean Squared Error)
# 1. MSEì— rootì„ ì·¨í•´ ì¤€ ê²ƒ
# 2. ì°¸ê°’ê³¼ ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì— í•´ì„ì´ ì‰¬ì›Œì§
# ğŸ‘ ë³´í†µ quadratic(2ì°¨ ê³¡ì„ í˜•íƒœ) í˜•íƒœì˜ ë¯¸ë¶„ í¸ì˜ì„±ì´ ì¢‹ê¸° ë•Œë¬¸ì—, íšŒê·€ ëª¨í˜•ì˜ ë¹„ìš©í•¨ìˆ˜ë¡œ MSEë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤.
def rmse(y_true, predictions):
  y_true, predictions = np.array(y_true), np.array(predictions)
  return np.sqrt(mse(y_true, prediction)) # ROOTed MSE

RMSE = rmse(Y_true, Y_pred)
RMSE

# %%
# - ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ìµœì í™” ë°©ë²•
# 1.ì •ê·œë°©ì •ì‹

# ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” Î¸ ê°’ì„ ì°¾ê¸° ìœ„í•œ í•´ì„ì  ë°©ë²•
# ì •ê·œë°©ì •ì‹ì€ nê°œì˜ íŠ¹ì„±ìˆ˜ì— ë”°ë¼ì„œ (n+1) x (n+1)ì˜ X XT ì—­í–‰ë ¬ì„ ê³„ì‚°í•œë‹¤.
# ì´ ë§ì€ íŠ¹ì„±ì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´ ì •ê·œë°©ì •ì‹ì˜ êµ¬í˜„ì†ë„ê°€ ëŠë ¤ì§„ë‹¤.
# í•˜ì§€ë§Œ ë‹¤í–‰íˆë„ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ í›ˆë ¨ ì„¸íŠ¸ì˜ ìƒ˜í”Œ ìˆ˜ì™€ íŠ¹ì„± ìˆ˜ì— ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•œë‹¤.
# ë©”ëª¨ë¦¬ ê³µê°„ì´ ì¶©ë¶„í•˜ë‹¤ë©´ í° í›ˆë ¨ ì„¸íŠ¸ë„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
# ë¹„ìš©í•¨ìˆ˜(MSE)ë¥¼ wì— ëŒ€í•´ ë¯¸ë¶„ì„ í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤.

# %%
# ë°ì´í„° ìƒì„±
x_data = 2 * np.random.rand(100,1) # 0 ~ 1 ë²”ìœ„ì—ì„œ ê· ì¼í•œ ë¶„í¬ 100 X 1 array
y_data = 4 + 3*x_data + np.random.randn(100,1) # normal distribution(mu=0, var=1)ë¶„í¬, 100 X 1 array

# %%
x_data
# %%
y_data

# %%
plt.scatter(x_data, y_data)
plt.show()

# %%
x_bias = np.c_[np.ones((100,1)),x_data] # ëª¨ë“  ìƒ˜í”Œì— index 0ë²ˆì— 1ì„ ì¶”ê°€
x_bias

# %%
# np.linalg.invëŠ” ë„˜íŒŒì´ ì„ í˜•ëŒ€ìˆ˜ ëª¨ë“ˆ(linalg)ì˜ inv(ì—­í•¨ìˆ˜)
# .dot()ì€ í–‰ë ¬ ê³±ì…ˆ
theta_best = np.linalg.inv(x_bias.T.dot(x_bias)).dot(x_bias.T).dot(y_data)
theta_best

# %%
# theta_bestë¥¼ ì‚¬ìš©í•´ì„œ y ê°’ ì˜ˆì¸¡
x_new = np.array([[0],[2]])
x_new_bias = np.c_[np.ones((2,1)) ,x_new]
prediction = x_new_bias.dot(theta_best)
prediction

# %%
plt.plot(x_new, prediction,"r-")
plt.plot(x_data,y_data,"b.")
plt.axis([0,2,0,15]) # xì¶• ë²”ìœ„ 0~2, yì¶• ë²”ìœ„ 0~15
plt.show()

# %%
# ìœ„ì™€ ê°™ì€ ì‘ì—…ì„ ì•„ë˜ì™€ ê°™ì´ sklearn(ì‚¬ì´í‚·ëŸ°) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬
# êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. (ì¤‘ìš”)
# =============================================================
from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()
linear_regression.fit(x_data, y_data)
prediction_new = linear_regression.predict(x_new) # ê°„ë‹¨í•˜ê²Œ predictionì„ êµ¬í•  ìˆ˜ ìˆë‹¤
# print(linear_regression.intercept_,linear_regression.coef_)

plt.plot(x_new, prediction_new,"r-")
plt.plot(x_data,y_data,"b.")
plt.axis([0,2,0,15]) # xì¶• ë²”ìœ„ 0~2, yì¶• ë²”ìœ„ 0~15
plt.show()

# %%
print(prediction_new) # scikitì„ í†µí•´ êµ¬í•œ ì˜ˆì¸¡ê°’
# =============================================================

# %%
# ì„ í˜•íšŒê·€ ëª¨ë¸ì˜ ìµœì í™” ë°©ë²•

# 2.ê¸°ìš¸ê¸° í•˜ê°•ë²• (gradient descent)
# loss function : ë¹„ìš© í•¨ìˆ˜
# step(learning rate) : í•™ìŠµë¥ 
# global minimum : ìµœì  ìµœì†Œê°’
# local minium : ì§€ì—­ ìµœì†Œê°’

# ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë¬¸ì œì—ì„œ ìµœì ì˜ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ë§¤ìš° ì¼ë°˜ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜
# ê¸°ë³¸ ë©”ì»¤ë‹ˆì¦˜ì€ ì§€ì •í•œ ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ê²ƒ

# ì§™ì€ ì•ˆê°œ ì†, ì•ì´ ì „í˜€ ë³´ì´ì§€ ì•Šê³  ì˜¤ë¡œì§€ ë°œ ëì— ì‚°ì˜ ê¸°ìš¸ê¸°ë§Œ ëŠë‚„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•´ë³´ì.
# ì´ ìˆ²ì„ ë²—ì–´ë‚˜ê¸° ìœ„í•œ ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ ê°€ì¥ ê¸°ìš¸ê¸°ê°€ ê¸‰í•œ ê¸¸ì„ ë”°ë¼ì„œ ë‚´ë ¤ê°€ëŠ” ê²ƒì´ë‹¤.
# ì´ê²ƒì´ ê²½ì‚¬ í•˜ê°•ë²• ì›ë¦¬ë‹¤.

# ë¬´ì‘ìœ„ë¡œ ë²¡í„° Î¸ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
# íŒŒë¼ë¯¸í„° ë²¡í„° Î¸ì— ëŒ€í•´ ë¹„ìš© í•¨ìˆ˜(Loss Function)ì˜ í˜„ì¬ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•œë‹¤.
# ê·¸ë¦¬ê³  ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•˜ë©´ì„œ,
# ìµœì¢…ì ìœ¼ë¡œ ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ë˜ë©´ ìµœì†Ÿê°’ì— ë„ë‹¬í•˜ë„ë¡ í•´ì•¼ í•œë‹¤.

# ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ìµœì í™” ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ê²Œ í•˜ëŠ”
# ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° step(learning rate)ë¥¼ ê²°ì •í•´ì•¼ í•œë‹¤.

# ì‹¤ì œ ëª¨ë“  ë¹„ìš©í•¨ìˆ˜ëŠ” ìœ„ì™€ ê°™ì´ quadratic(ì´ì°¨ì›)í•˜ê²Œ í‘œí˜„ë˜ì§€ ì•Šê³ 
# ìš¸ê¸‹ë¶ˆê¸‹í•˜ê²Œ ì†Ÿì•˜ë‹¤ê°€ ë‚´ë ¤ì•‰ì•˜ë‹¤ê°€ í•œë‹¤.

# í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš°ì—ëŠ” local minumum(ì§€ì—­ ìµœì†Œê°’)ì— ë¹ ì§€ê²Œ ëœë‹¤.
# í•™ìŠµë¥ ì´ ë„ˆë¬´ í° ê²½ìš°ì—ëŠ” ìˆ˜ë ´ì´ ë˜ì§€ ì•Šê²Œ ëœë‹¤.

# ê·¸ë˜ì„œ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ í•™ìŠµë¥ ì„ ì˜ ì¡°ì •í•´ì•¼ì§€,
# global minumum(ìµœì ì˜ ê°’)ìœ¼ë¡œ ì˜ ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤.

# ë‹¤í–‰íˆë„ ì„ í˜• íšŒê·€ì˜ MSE ë¹„ìš© í•¨ìˆ˜ëŠ”
# convex function(ë³¼ë¡ í•¨ìˆ˜)ì´ê¸° ë•Œë¬¸ì—,
# local minumì´ ì—†ê³ , global minimumë§Œ ì¡´ì¬í•œë‹¤.

# ê·¸ë˜ì„œ ì¶©ë¶„í•œ ì‹œê°„ê³¼ ì ì ˆí•œ í•™ìŠµë¥ ë§Œ ì£¼ì–´ì§„ë‹¤ë©´,
# global minimumì— ìµœëŒ€í•œ ê·¼ì ‘í•  ìˆ˜ ìˆë‹¤.

# ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ì˜¤ë¥¸ìª½ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì€ ê³§ì¥ global minimumìœ¼ë¡œ ë‚´ë ¤ê°ˆ ìˆ˜ ìˆë‹¤.

# ì™¼ìª½ì˜ ê·¸ë¦¼ì—ì„œ ì™„ë§Œí•œ ê²½ì‚¬ë¥¼ ë§Œë‚˜ê²Œ ë˜ë©´ global minimumìœ¼ë¡œ ë‚´ë ¤ê°ˆ ìˆ˜ëŠ” ìˆì§€ë§Œ ë” ì˜¤ë˜ ê±¸ë¦°ë‹¤.

# ê²½ì‚¬ í•˜ê°•ë²• ì „ì—ëŠ” ë°˜ë“œì‹œ ëª¨ë“  íŠ¹ì„±ì„ ê°™ì€ ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í•˜ì—¬ì„œ
# ë°ì´í„° ë³€í™˜ì„ í•˜ì—¬ì•¼ í•œë‹¤.

# %%
# =======================================================
# scikit learn(ì‚¬ì´í‚·ëŸ°) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ
# ê° íŠ¹ì„±ì—ì„œ í‰ê· ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´
# í‰ê· ì„ 0 ë¶„ì‚°ì„ 1ë¡œë§Œë“œëŠ” StandardScalerì„ ì‚¬ìš©í•˜ê³¤ í•œë‹¤.
# =======================================================

# í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
X, _ = make_blobs(n_samples= 200, centers= 5, random_state=4, cluster_std=1.5)
plt.scatter(X[:,0],X[:,1])
X_train, X_test = train_test_split(X, random_state=5, test_size=.1)

# ë©”ì†Œë“œì²´ì´ë‹(chaining)ì„ ì‚¬ìš©í•˜ì—¬ fitê³¼ transformì„ ì—°ë‹¬ì•„ í˜¸ì¶œí•©ë‹ˆë‹¤
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit(X_train).transform(X_train)

# ìœ„ì™€ ë™ì¼í•˜ì§€ë§Œ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤(fit_transform)
X_scaled_d = scaler.fit_transform(X_train)

#í•´ë‹¹ fitìœ¼ë¡œ testë°ì´í„°ë„ transform í•´ì¤ë‹ˆë‹¤
X_test_scaled = scaler.transform(X_test)

plt.scatter(X_scaled[:,0],X_scaled[:,1])

# %%
# âœ” ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• (ê° epochë§ˆë‹¤ ì „ì²´ ë°ì´íƒ€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•)

# ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ê° ëª¨ë¸ì˜ Î¸jì— ëŒ€í•œ ë¹„ìš© í•¨ìˆ˜ì˜ partial derivative(í¸ë¯¸ë¶„) ê°’ì„ 1ë²ˆ ì²˜ëŸ¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.
# partial derivativeë¥¼ ê°ê° ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  2ë²ˆì²˜ëŸ¼ í•œë²ˆì— ê³„ì‚°ë„ ê°€ëŠ¥í•˜ë‹¤.
# ìœ„ ê³µì‹ì€ ë§¤ ê²½ì‚¬ í•˜ê°• ìŠ¤í…ì—ì„œ "ì „ì²´" í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•´ ê³„ì‚°í•œë‹¤.
# ê·¸ë˜ì„œ ì´ ê³µì‹ì„ Batch Gradient Descent(ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•)ì´ë¼ê³  í•œë‹¤.
# ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í° í›ˆë ¨ ì„¸íŠ¸ì—ì„œëŠ” ì•„ì£¼ ëŠë¦¬ë‹¤.
# ğŸˆ ì¥ì ìœ¼ë¡œëŠ”, íŠ¹ì„± ìˆ˜ì— ë¯¼ê°í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì •ê·œë°©ì •ì‹ë³´ë‹¤ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë¹ ë¥´ë‹¤.

# %%
# ê²½ì‚¬ í•˜ê°•ë²• êµ¬í˜„(implementation)
import numpy as np

x_data = 2 * np.random.rand(100,1) # 100 x 1 í¬ê¸°ì˜ 0~1ì˜ ê· ì¼ë¶„í¬
x_bias = np.c_[np.ones((100,1)),x_data] # bias(1)ë¥¼ ì „ì²´ ë°ì´í„°ì— ì¶”ê°€
y_data = 4 + 3*np.random.randn(100,1) # 100 x 1 í¬ê¸°ì˜ í‘œì¤€ì •ê·œë¶„í¬ ì¶”ì¶œ

learning_rate = 0.001
iterations = 2000
m = x_bias.shape[0] # 100ê°œ (x ë°ì´í„°)

theta = np.random.randn(2,1) # 2x1 í¬ê¸°ì˜ í‰ê·  0, ë¶„ì‚°1 ì •ê·œ ë¶„í¬ ì¶”ì¶œ
print('org:', theta)

for iteration in range(iterations):
  gradients = 2/m * x_bias.T.dot(x_bias.dot(theta)-y_data)
  theta = theta - (learning_rate * gradients)

# ì •ê·œë°©ì •ì‹ìœ¼ë¡œ ì°¾ì€ ê²ƒê³¼ ì •í™•íˆ ì¼ì¹˜í•œë‹¤.
print('learn:', theta)

# %%
# âœ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (stochastic gradient descent)
# Stochastic Gradient Descent (SGD)
# ìš©ì–´ : ë°˜ë³µ í•™ìŠµë¥  ê²°ì • í•¨ìˆ˜ => learning schedule(í•™ìŠµ ìŠ¤ì¼€ì¥´)

# ì•ì„œ, ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ì–¸ê¸‰í•œëŒ€ë¡œ ë§¤ ìŠ¤í…ì—ì„œ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•´ì•¼ í•˜ëŠ” í° ë¬¸ì œê°€ ìˆë‹¤.
# ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì€ ë§¤ ìŠ¤í…ë§ˆë‹¤ í•œ ê°œì˜ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ ë° ê·¸ ìƒ˜í”Œì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•œë‹¤.
# ë§¤ ë°˜ë³µì—ì„œ ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸ í•˜ê¸° ë•Œë¬¸ì— ìµœì í™”ê°€ ë” ë¹ ë¥´ë‹¤.
# ê·¸ë¦¬ê³  ì „ì²´ ë°ì´í„°ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œí•´ì„œ ìµœì í™” ì‹œí‚¤ê¸° ë•Œë¬¸ì— ë§¤ìš° í° í›ˆë ¨ ë°ì´í„° ì—­ì‹œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
# í•˜ì§€ë§Œ ë¬´ì‘ìœ„ ì¶”ì¶œì´ê¸° ë•Œë¬¸ì—, ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ ì•ˆì •ì ì´ì§€ëŠ” ëª»í•˜ë‹¤.
# ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë¹„ìš© í•¨ìˆ˜ì˜ global minumumì— ë„ë‹¬í•˜ê¸° ê¹Œì§€ ìš”ë™ì¹˜ë©° í‰ê· ì ìœ¼ë¡œ ê°ì†Œí•œë‹¤.
# ìš”ë™ì¹˜ë©´ì„œ ìµœì ì˜ í•´ì— ê°€ê¹Œì›Œì§€ê¸°ëŠ” í•˜ê² ì§€ë§Œ, ìµœì†Œê°’ì— ë„ë‹¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤.
# í•˜ì§€ë§Œ ë¹„ìš© í•¨ìˆ˜ê°€ MSEì²˜ëŸ¼ convex(ë³¼ë¡ í•¨ìˆ˜)í•˜ì§€ ì•Šê³  ë¶ˆê· í˜•í•˜ë‹¤ë©´ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ë³´ë‹¤ global minimumì— ë„ë‹¬í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.
# ë¬´ì‘ìœ„ì„±ìœ¼ë¡œ ì¸í•œ global minimumì— ë„ë‹¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ, í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ëŠ” í•´ê²°ì±…ì´ ìˆë‹¤.(ì‹œì‘: í•™ìŠµë¥  í¬ê²Œ => ì§„í–‰ë‹¨ê³„: í•™ìŠ¬ë¥  ì‘ê²Œ)
# ìœ„ í•´ê²°ì±…ì„ ìœ„í•œ ë§¤ ë°˜ë³µ í•™ìŠµë¥  ê²°ì • í•¨ìˆ˜ë¥¼ learning schedule(í•™ìŠµ ìŠ¤ì¼€ì¥´)ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

# %%
# í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• êµ¬í˜„(implementation)
epochs = 1000
t0,t1 = 5, 50 # í•™ìŠµ ìŠ¤ì¼€ì¥´ (í•˜ì´í¼ íŒŒë¼ë¯¸í„°)
m = x_bias.shape[0] # 100ê°œ (x ë°ì´í„°)

def learning_schedule(t):
  return t0 / (t+t1)

theta = np.random.randn(2,1) # 2x1 í¬ê¸°ì˜ í‰ê·  0, ë¶„ì‚°1 ì •ê·œ ë¶„í¬ ì¶”ì¶œ

for epoch in range(epochs):
  for i in range(m):
    random_index = np.random.randint(m) # 0 ~ m-1ê¹Œì§€ ëœë¤ ìˆ«ì 1
    xi = x_bias[random_index:random_index:+1] # 1 x 2 í¬ê¸°
    yi = y_data[random_index:random_index+1] # 1 x 1 í¬ê¸°
    gradients = 2 * xi.T.dot(xi.dot(theta)-yi) # 1 => mini_m
    learning_rate = learning_schedule(epoch*m + i)
    theta = theta - learning_rate * gradients

# %%
# https://scikit-learn.org/stable/modules/sgd.html
# Stochastic Gradient Descent (SGD)
from sklearn.linear_model import SGDClassifier
X = [[0., 0.], [1., 1.]]
y = [0, 1]
X

# %%
clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
clf.fit(X, y)
SGDClassifier(max_iter=5)

# %%
clf.predict([[2., 2.]])

# %%
clf.coef_

# %%
clf.intercept_

# %%
[[2., 2.]]

# %%
clf.decision_function([[2., 2.]])

clf = SGDClassifier(loss="log_loss", max_iter=5).fit(X, y)
clf.predict_proba([[1., 1.]])

# %%
# âœ” ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•

# ë¯¸ë‹ˆë°°ì¹˜ëŠ” ìœ„ ë°°ì¹˜ì™€ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ì•ˆë‹¤ë©´ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.
# ê° Stepì—ì„œ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì¼ì¼ì´ ë‹¤ í•™ìŠµí•˜ëŠ” ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì´ë‚˜
# í›ˆë ¨ ì„¸íŠ¸ì˜ í•˜ë‚˜ì˜ ìƒ˜í”Œì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼,
# Mini Batchë¼ ë¶€ë¥´ëŠ” ì„ì˜ì˜ ì‘ì€ ìƒ˜í”Œ ì„¸íŠ¸ì— ëŒ€í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.

# í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì— ë¹„í•´ í–‰ë ¬ ì—°ì‚°ì— ìµœì í™”ëœ GPUë¥¼ ì‚¬ìš©í•´ì„œ ì„±ëŠ¥ì„ ë” ì˜¬ë¦´ ìˆ˜ ìˆë‹¤.
# ë¯¸ë‹ˆ ë°°ì¹˜ê°€ í° ê²½ìš°, íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ SGD(í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)ë³´ë‹¤ ëœ ë¶ˆê·œì¹™í•˜ê²Œ ì›€ì§ì¸ë‹¤.
# ê³§, SGDë³´ë‹¤ ìµœì†Œê°’ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì´ ë” ë†’ì§€ë§Œ, Local Minimum(êµ­ì†Œê°’)ì— ë¹ ì§ˆ ìœ„í—˜ì€ ì¡´ì¬í•œë‹¤.

# ìœ„ ê·¸ë¦¼ì€ êµ­ì†Œê°’ì´ ë§ê³  ë¬¸ì œê°€ ë˜ëŠ” íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ ë°°ì¹˜, ë¯¸ë‹ˆë°°ì¹˜, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.
# ëª¨ë‘ ìµœì†Œê°’ì— ë„ë‹¬í•˜ì˜€ì§€ë§Œ, ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì´ ì‹¤ì œ ìµœì†Œê°’ì—ì„œ ë„ë‹¬í•˜ì˜€ê³ , ë‚˜ë¨¸ì§€ ë‘ ë°©ë²•ì€ ê·¼ì²˜ë¥¼ ë§´ëŒê³  ìˆë‹¤.
# ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì´ ìµœì†Œê°’ì— ë„ë‹¬í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, ë§¤ ìŠ¤í…ì—ì„œ ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì´ ë“ ë‹¤.
# í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ë¯¸ë‹ˆ ë°°ì¹˜ëŠ” ì ì ˆíˆ í•™ìŠµ ìŠ¤ì¼€ì¥´ì„ ì‚¬ìš©í•œë‹¤ë©´ ìµœì†Œê°’ì— ë§ˆì°¬ê°€ì§€ë¡œ ë„ë‹¬ í•  ìˆ˜ ìˆë‹¤.

# ì‚¬ì´í‚·ëŸ°ì˜ SGDRegressorì™€ SGDClassifierì—ì„œ partial_fit ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•Šê³  ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì„ ìœ„í•´ ë°˜ë³µì ìœ¼ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ partial_fit ë©”ì„œë“œëŠ” fit ë©”ì„œë“œì™€ ë™ì¼í•˜ê²Œ ë¯¸ë‹ˆë°°ì¹˜ì˜ ìƒ˜í”Œì„ í•˜ë‚˜ì”© ì ìš©í•˜ë¯€ë¡œ ì—„ë°€íˆ ë§í•˜ë©´ ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë‹ˆë‹¤. - í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ -

# =======================================
# sklearn ì—ì„œ ì§€ì›í•˜ëŠ” íšŒê·€ í•¨ìˆ˜ (ì¤‘ìš”)
# =======================================
# sklearn.LinearRegression (ì •ê·œë°©ì •ì‹)
# sklearn.SGDRegressor (í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)
# =======================================

# %%
# - ë‹¤í•­ íšŒê·€
# íŠ¹ì„±ê³µí•™ ê¸°ë²• í™œìš©

# ë¹„ì„ í˜•ì„±ì„ ë„ëŠ” ë°ì´í„°ë„ ì„ í˜• ëª¨ë¸ì„ í™œìš©í•˜ì—¬ì„œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.
# ê¸°ì¡´ íŠ¹ì„±ì—ë‹¤ê°€ log, exp, ì œê³± ë“±ê³¼ ê°™ì€ basis functionì„ ì ìš©í•˜ì—¬,
# í™•ì¥ëœ íŠ¹ì„±ì„ í¬í•¨í•œ í˜•íƒœë¡œ ë³€í˜•í•œ ë’¤ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ë‹¤í•­ íšŒê·€ ê¸°ë²•ì´ë¼ê³  í•œë‹¤.
# ìœ„ì™€ ê°™ì€ ì˜ˆì‹œë¥¼ 2ì°¨ ë°©ì •ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ë“¤ì–´ ë³´ê² ë‹¤.

# %%
import numpy as np
data_num = 1000
x_data = 3 * np.random.rand(data_num,1) - 1
x_data

# %%
y_data = 0.2 * (x_data**2) + np.random.randn(1000,1)
y_data

# %%
# 1ì°¨ ë°©ì •ì‹ : y = ax
# nì°¨ ë°©ì •ì‹ : y = ax + ... + ax^n
# PolynomialFeatures : nì°¨ ë°©ì •ì‹ì„ í¬í•¨í•˜ëŠ” íŠ¹ì„±ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x_data)

# %%
print(x_data[0])
# %%
print(x_poly[0])

# %%
from sklearn.linear_model import LinearRegression
linear_regression = LinearRegression()
linear_regression.fit(x_poly,y_data)
print(linear_regression.intercept_,linear_regression.coef_)

# ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹ì€ y_hat = 0.25x^2 - 0.05x^1 + 1 ì´ë‹¤.
# ì‹¤ì œ ì›ë˜ í•¨ìˆ˜ì™€ ê±°ì˜ ë¹„ìŠ·í•´ì¡Œë‹¤.

# íŠ¹ì„±ì´ ì—¬ëŸ¬ ê°œ ì¼ ë•Œ ë‹¤í•­ íšŒê·€ëŠ” ì´ íŠ¹ì„± ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# (PolynomialFeaturesë¥¼ í†µí•´ì„œ ì£¼ì–´ì§„ ì°¨ìˆ˜ê¹Œì§€ íŠ¹ì„± ê°„ì˜ ëª¨ë“  êµì°¨í•­ì„ ì¶”ê°€í•  ìˆ˜ ìˆê¸° ë•Œë¬¸)

# í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ ë³´ëŠ” ê²ƒ,
# (ëª¨ë¸ ê³¼ì í•©ì„ ê°€ì‹œì ìœ¼ë¡œ í™•ì¸ í•˜ëŠ” ë²•)


# %%
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def plot_learning_curves(model,x,y):
  x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2)
  train_errors,val_errors = [],[]
  for num in range(1,len(x_train)):
    model.fit(x_train[:num],y_train[:num])
    y_train_predict = model.predict(x_train[:num])
    y_val_predict = model.predict(x_val)
    train_errors.append(mean_squared_error(y_train[:num],y_train_predict))
    val_errors.append(mean_squared_error(y_val,y_val_predict))
  plt.plot(np.sqrt(train_errors),'r-+',linewidth=2,label='train_set')
  plt.plot(np.sqrt(val_errors),'b-',linewidth=3,label='val_set')
  plt.legend()
  plt.show()

plot_learning_curves(linear_regression,x_poly,y_data)

# %%
# basis function(4ì°¨ ë‹¤í•­ì‹)ê³¼ ëª¨ë¸ì„
# íŒŒì´í”„ë¼ì¸ì„ ê±°ì³ì„œ (ì¤‘ìš”)
# ë°”ë¡œ ëª¨ë¸ë§ ë° í‰ê°€í•˜ëŠ” ë²•
from sklearn.pipeline import Pipeline
data_num = 100
x_data = 3 * np.random.rand(data_num,1) - 1
y_data = 0.2 * x_data**2 + np.random.randn(100,1)
polynomial_regression = Pipeline([
  ("poly_features", PolynomialFeatures(degree=4, include_bias=False)),
  ("lin_reg",LinearRegression())
  ])

plot_learning_curves(polynomial_regression,x_data,y_data)

# %%
# - ê·œì œê°€ ìˆëŠ” ì„ í˜• ëª¨ë¸
# ê·œì œ : Regularization

# ì•ì—ì„œë„ ë³´ì•˜ë“¯ì´ ê³¼ëŒ€ì í•©ì„ ê°ì†Œì‹œí‚¤ëŠ” ê²ƒì´ ì•„ì£¼ ì¤‘ìš”í•˜ê³ , ê·¸ ê³¼ëŒ€ì í•©ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ê°•êµ¬í•´ì•¼ í•œë‹¤.
# ê·¸ ë°©ë²• ì¤‘ lossì— ìƒˆë¡œìš´ termì„ ì¶”ê°€í•˜ì—¬ì„œ ëª¨ë¸ weightì— ëŒ€í•œ ê·œì œë¥¼ ê°€í•˜ëŠ” ê²ƒì´ë‹¤.
# Lasso Regression (ë¼ì˜ íšŒê·€)
# Lasso(ë¼ì˜)íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì˜ ê·œì œëœ ë²„ì „ì´ë‹¤.
# ë¹„ìš© í•¨ìˆ˜ì— L1 term(ê°€ì¤‘ì¹˜ì— ì ˆëŒ€ê°’ì„ ì ìš©í•œ í˜•íƒœ)ì„ ì‚¬ìš©í•˜ì—¬ì„œ weightì— ê·œì œë¥¼ í•œë‹¤.
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(x_data,y_data)
lasso_reg.predict([[1.5]])

# %%
# Ridge Regression (ë¦¿ì§€ íšŒê·€)
# Ridge(ë¦¿ì§€)íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì˜ ê·œì œëœ ë²„ì „ì´ë‹¤.
# ë¹„ìš© í•¨ìˆ˜ì— L2 term(ê°€ì¤‘ì¹˜ì— ì œê³±ì„ í•œ í˜•íƒœ)ì„ ì‚¬ìš©í•˜ì—¬ weightì— ê·œì œë¥¼ í•œë‹¤.
# ì•ˆë“œë ˆ ë£¨ì´ ìˆ„ë ˆìŠ¤í‚¤ê°€ ë°œê²¬í•œ í–‰ë ¬ ë¶„í•´(matrix factorization) ì‚¬ìš©
# ìˆ„ë ˆìŠ¤í‚¤ ë¶„í•´ì˜ ì¥ì ì€ ì„±ëŠ¥ì´ë‹¤. ì›ë˜ ridgeì˜ solver defaultê°’ì€ 'auto'ì´ë©° í¬ì†Œ í–‰ë ¬ì´ë‚˜ íŠ¹ì´ í–‰ë ¬ì´ ì•„ë‹ˆë©´ 'cholesky'ê°€ ëœë‹¤.

from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=0.1, solver='cholesky')
ridge_reg.fit(x_data,y_data)
ridge_reg.predict([[1.5]])

# Lasso Regression (ë¼ì˜ íšŒê·€)
# Ridge Regression (ë¦¿ì§€ íšŒê·€) => ë” ë§ì´ ì‚¬ìš©

# %%
# Elastic Net Regression (ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€)
# Lasso Regression(ë¼ì˜ íšŒê·€)ì™€ Ridge Regression(ë¦¿ì§€ íšŒê·€)ì˜ (ì§¬ë½•)í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.

# Elastic Net(ì—˜ë¼ìŠ¤í‹±ë„·)ì€ ë¦¿ì§€ íšŒê·€ì™€ ë¼ì˜ íšŒê·€ë¥¼ í•©ì„±í•œ ëª¨ë¸ì´ë‹¤.
# ê·œì œ termì€ ë¦¿ì§€íšŒê·€ì˜ ê·œì œtermê³¼ ë‹¨ìˆœíˆ ë”í•´ì„œ ì‚¬ìš©í•˜ê³ , í˜¼í•© ë¹„ìœ¨ rì„ ì¡°ì ˆí•´ì„œ ì‚¬ìš©í•œë‹¤.
# r=0ì´ë©´, ì—˜ë¼ìŠ¤í‹± ë„·ì€ => ë¦¿ì§€ íšŒê·€
# r=1ì´ë©´, ì—˜ë¼ìŠ¤í‹± ë„·ì€ => ë¼ì˜ íšŒê·€
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5)
elastic_net.fit(x_data,y_data)
elastic_net.predict([[1.5]])

# - Early Stopping (ì¡°ê¸° ì¢…ë£Œ) ==> ë§¤ìš° ì¤‘ìš”
# ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì€ ë°˜ë³µì ì¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ê·œì œí•˜ëŠ” ê²ƒì€
# ì‹¤ì œ ë¹„ìš©í•¨ìˆ˜ì— ê·œì œí•­ì„ ì¶”ê°€í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼,
# ê²€ì¦ ì—ëŸ¬ê°€ í›ˆë ¨ ì—ëŸ¬ì— ë¹„í•´ ì¹˜ì†Ÿê²Œ ë˜ë©´ ë°”ë¡œ í›ˆë ¨ì„ ì¤‘ì§€ì‹œí‚¤ëŠ” ë°©ë²•ë„ ìˆë‹¤.
# ì´ë¥¼ early stoppingì´ë¼ í•œë‹¤.

from sklearn.base import clone
from sklearn.preprocessing import PolynomialFeatures,StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error

x_data = 3 * np.random.rand(data_num,1) - 1
y_data = 0.2 * x_data**2 + np.random.randn(100,1)

poly_scaler = Pipeline([
                        ("poly_features",PolynomialFeatures(degree=90,include_bias=False)),
                        ('std_scaler',StandardScaler())
])

x_train,x_val,y_train,y_val = train_test_split(x_data,y_data,test_size=0.2)
x_train_poly_scaled = poly_scaler.fit_transform(x_train)
x_val_poly_scaled = poly_scaler.transform(x_val)

# warm_start=True ì´ë©´ fit ë©”ì„œë“œê°€ í˜¸ì¶œë  ë•Œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•˜ì§€ ì•Šê³  ì´ì „ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì—ì„œ í›ˆë ¨ ì´ì–´ì§
# penalty : {â€˜l2â€™, â€˜l1â€™, â€˜elasticnetâ€™}, default=â€™l2â€™
# n_iter_no_change : Number of iterations with no improvement to wait before stopping fitting
# 'constant' : eta = eta0
# 'optimal' : eta = 1.0 / (alpha * (t + t0))
# 'invscaling' : eta = eta0 / pow(t, power_t)
# 'adaptive' : eta = eta0, as long as the training keeps decreasing
sgd_reg = SGDRegressor(n_iter_no_change=10,
                       warm_start=True,
                       penalty=None,
                       learning_rate='constant',
                       eta0=0.0005
                       )
minimum_val_error = float('inf')
best_epoch = None
best_model = None
for epoch in range(1000):
  sgd_reg.fit(x_train_poly_scaled,y_train.ravel())
  y_val_predict = sgd_reg.predict(x_val_poly_scaled)
  val_error = mean_squared_error(y_val,y_val_predict)
  if val_error<minimum_val_error:
    minimum_val_error = val_error
    best_epoch = epoch
    best_model = clone(sgd_reg)
print('best_epoch : ',best_epoch)
print('best_model : ',best_model)
